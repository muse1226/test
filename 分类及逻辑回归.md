

# 分类

## 概率生成模型

两步：

1. 找一个function set   

   ![1557286344157](C:\Users\ADMINI~1\AppData\Local\Temp\1557286344157.png)

   2.评估function的好坏，就是找一个概率模型（比如高斯分布），从该分布采样的点概率最大，求得使得这些点概率最大的$\mu,\Sigma$，找到这个高斯分布后，求某个X被这个高斯分布采样出来的概率

   总结：

   简单来说，就是根据给的训练样本，找到采样这些训练样本的概率分布，例如高斯分布，并且该高斯分布要最大可能的取得这些训练样本。然后，给一个新的测试样本X，预测它被该高斯分布采样的概率，若大于0.5，则x属于该类

   注意：

   当两类高斯分布的协方差相同时，分界线为直线，并且预测正确率高。

   两个特征X1,X2之间是独立的，所以协方差矩阵除了对角线都是0


### 全概率公式

对一个试验的样本空间s进行划分，分成B1,B2,B3.......,并且子空间之间没有交集，例如摇色子，样本空间为{1,2,3,4,5,6},B1{1,2,3},B2={4,5},B3={6}
$$
\frac{P(A)=P\left(A | B_{1}\right) P\left(B_{1}\right)+P\left(A | B_{2}\right) P\left(B_{2}\right)+\cdots+}{P\left(A | B_{n}\right) P\left(B_{n}\right)}
$$
上式称为全概率公式，一般P(A)不好求时，可以用全概率公式

### 贝叶斯公式


$$
\begin{array}{l}{P\left(B_{i} | A\right)=\frac{P\left(B_{i} A\right)}{P(A)}=\frac{P\left(A | B_{i}\right) P\left(B_{i}\right)}{\sum_{i=1}^{n} P\left(A | B_{j}\right) P\left(B_{j}\right)}, i=1,2, \cdots, n}\end{array}
$$

### 问题

我们要知道一只宝可梦是类别1的概率，就需要知道下图四个红框的概率

![1557220521122](C:\Users\ADMINI~1\AppData\Local\Temp\1557220521122.png)

![1557220653969](C:\Users\ADMINI~1\AppData\Local\Temp\1557220653969.png)

**分成水类和正常，训练数据中79只是水族，61正常**



![1557220802830](C:\Users\ADMINI~1\AppData\Local\Temp\1557220802830.png)



求在水族中某一只宝可梦的概率，我们已知79只水族的特征

![1557220941359](C:\Users\ADMINI~1\AppData\Local\Temp\1557220941359.png)

假设这79只水族的宝可梦二维特征都服从高斯分布，这79只是从高斯分布采样出来的

### 高斯分布

**标准差**  之所以除以n-1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。 

$s=\sqrt{\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}}{n-1}}$

我们应该注意到，标准差和方差一般是用来描述一维数据的，但现实生活我们常常遇到含有多维数据的数据集，最简单的大家上学时免不了要统计多个学科的考试成绩。面对这样的数据集，我们当然可以按照每一维独立的计算其方差，但是通常我们还想了解更多，比如，一个男孩子的猥琐程度跟他受女孩子欢迎程度是否存在一些联系啊，嘿嘿~协方差就是这样一种用来度量两个随机变量关系的统计量，我们可以仿照方差的定义： 

仿照方差的定义：

 ![img](http://images.cnitblog.com/blog/397158/201307/24152520-57efb2d1a89446f1ac4691a88bea7d8e.jpg)

来度量各个维度偏离其均值的程度，标准差可以这么来定义：

**协方差**

$\operatorname{cov}(X, Y)=\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(Y_{i}-\overline{Y}\right)}{n-1}$

协方差的结果有什么意义呢？如果结果为正值，则说明两者是正相关的(从协方差可以引出“相关系数”的定义)，也就是说一个人越猥琐就越受女孩子欢迎，嘿嘿，那必须的~结果为负值就说明负相关的，越猥琐女孩子越讨厌，可能吗？如果为0，也是就是统计上说的“相互独立”。协方差为0不一定“相互独立”，只能说明不相关 不相关不一定独立，而独立一定不相关  

**协方差矩阵**

协方差也只能处理二维问题，那维数多了自然就需要计算多个协方差，比如n维的数据集就需要计算 n! / ((n-2)!*2) 个协方差，那自然而然的我们会想到使用矩阵来组织这些数据。给出协方差矩阵的定义： 

例如我们可以举一个简单的三维的例子，假设数据集有三个维度，则协方差矩阵为 

$C=\left( \begin{array}{ccc}{\operatorname{cov}(x, x)} & {\operatorname{cov}(x, y)} & {\operatorname{cov}(x, z)} \\ {\operatorname{cov}(y, x)} & {\operatorname{cov}(y, y)} & {\operatorname{cov}(y, z)} \\ {\operatorname{cov}(z, x)} & {\operatorname{cov}(z, y)} & {\operatorname{cov}(z, z)}\end{array}\right)$

**二维高斯分布**

**为了简单起见，本文假设所有变量都是相互独立的** 

f(x0,x1,…,xn)=f(x0)f(x1)f(xn) 

一维此时变二维向量

$\overline{x}=\left[ \begin{array}{l}{x_{1}} \\ {x_{2}}\end{array}\right]$

$\overline{u}=\left[ \begin{array}{l}{u_{1}} \\ {u_{2}}\end{array}\right]$

$\overline{\sigma}=\left[ \begin{array}{l}{\sigma_{1}} \\ {\sigma_{2}}\end{array}\right]$

由于 x1x1，x2x2 是相互独立的，所以，$\bar{x}$的高斯分布函数可以表示为 
$$
\begin{aligned} f(\overline{x}) &=f\left(x_{1}, x_{2}\right) \\ &=f\left(x_{1}\right) f\left(x_{2}\right) \\ &=\frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} \exp \left(-\frac{1}{2}\left(\frac{x_{1}-u_{1}}{\sigma_{1}}\right)^{2}\right) \times \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} \exp \left(-\frac{1}{2}\left(\frac{x_{2}-u_{2}}{\sigma_{2}}\right)^{2}\right) \\ &=\frac{1}{(2 \pi)^{2 / 2}\left(\sigma_{1}^{2} \sigma_{2}^{2}\right)^{1 / 2}} \exp \left(-\frac{1}{2}\left[\left(\frac{x_{1}-u_{1}}{\sigma_{1}}\right)^{2}+\left(\frac{x_{2}-u_{2}}{\sigma_{2}}\right)^{2}\right]\right) \end{aligned}
$$
对于二维的向量 x¯¯¯x¯ 而言，其协方差矩阵为： 
$$
\begin{aligned} \Sigma &=\left[ \begin{array}{ll}{\sigma_{11}} & {\sigma_{12}} \\ {\sigma_{12}} & {\sigma_{22}}\end{array}\right] \\ &=\left[ \begin{array}{ll}{\sigma_{1}^{2}} & {\sigma_{12}} \\ {\sigma_{21}} & {\sigma_{2}^{2}}\end{array}\right] \end{aligned}
$$


由于 x1x1，x2x2 是相互独立的，所以 σ12=σ21=0σ12=σ21=0。这样，Σ退化成$\left[ \begin{array}{cc}{\sigma_{1}^{2}} & {0} \\ {0} & {\sigma_{2}^{2}}\end{array}\right]$

最终，一维和二维高斯公式如下,D是维度数：
$$
\begin{array}{c}{N\left(x | u, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{1}{2 \sigma^{2}}(x-u)^{2}\right]} \\ {N(\overline{x} | \overline{u}, \Sigma)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma|^{1 / 2}} \exp \left[-\frac{1}{2}(\overline{x}-\overline{u})^{T} \Sigma^{-1}(\overline{x}-\overline{u})\right]}\end{array}
$$

### Probability from Class

假如这79只宝可梦是从一个高斯分布中采样出来的，我们现在要根据这79只去推算出来$\mu,\Sigma$

，然后得到一个高斯分布，给一个新的X，就可以预估它被这个高斯分布采样出来的概率

### Maximum Likelihood

如何找$\mu,\Sigma$

![1557284057348](C:\Users\ADMINI~1\AppData\Local\Temp\1557284057348.png)

选择不同的高斯分布，选出79个点的概率不同，则我们建立一个79个点被选出来的可能性为
$$
L(\mu, \Sigma)=f_{\mu, \Sigma}\left(x^{1}\right) f_{\mu, \Sigma}\left(x^{2}\right) f_{\mu, \Sigma}\left(x^{3}\right) \ldots \ldots f_{\mu, \Sigma}\left(x^{79}\right)
$$
 选择使L最大的$\mu,\Sigma$，要么就求两个参数的偏导，然后导数取0，求出最大的$\mu,\Sigma$

要么，带入下式

$\mu^{*}=\frac{1}{79} \sum_{n=1}^{79} x^{n}$最大



**现在我们可以做分类了**

![1557285278703](C:\Users\ADMINI~1\AppData\Local\Temp\1557285278703.png)

#### 重点！！！$\Sigma_1,\Sigma_2$在两个类别上应该相同！！！

实验证明，当$\Sigma_1,\Sigma_2$相同时，分类界线为一个直线，正确率提高了

![1557285827013](C:\Users\ADMINI~1\AppData\Local\Temp\1557285827013.png)

![1557285910581](C:\Users\ADMINI~1\AppData\Local\Temp\1557285910581.png)

![1557286102912](C:\Users\ADMINI~1\AppData\Local\Temp\1557286102912.png)

### 与逻辑回归的联系

我们最终的目的是求
$$
\begin{array}{l}{P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}} \\ {=\frac{1}{1+\frac{1}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}=\frac{1}{1+\exp (-z)}=\sigma(z)}\end{array}
$$

$$
z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}
$$

![1557302351775](C:\Users\ADMINI~1\AppData\Local\Temp\1557302351775.png)

$\sigma(z)$是Sigmoid function

最终化简得到
$$
P\left(C_{1} | x\right)=\sigma(w \cdot x+b)
$$

$$
z=w \cdot x+b=\sum_{i} w_{i} x_{i}+b
$$

在的$\Sigma1=\Sigma2$的前提下，z是一个线性函数，那么之前概率生成模型需要计算$\mu_1,\mu_2,\Sigma$，然而现在我们只需知道w,b就可以了

# 逻辑回归

![1557305224182](C:\Users\ADMINI~1\AppData\Local\Temp\1557305224182.png)

这件事就叫逻辑回归

### 交叉熵(cross entropy）

#### 信息量

$$
I\left(x_{0}\right)=-\log \left(p\left(x_{0}\right)\right)
$$

一个事件结果发生的概率越小，不确定性越大，信息量就越大

#### 熵

| 序号 | 事件         | 概率p | 信息量I         |
| ---- | ------------ | ----- | --------------- |
| A    | 电脑正常开机 | 0.7   | -log(p(A))=0.36 |
| B    | 电脑无法开机 | 0.2   | -log(p(B))=1.61 |
| C    | 电脑爆炸了   | 0.1   | -log(p(C))=2.30 |

熵用来表示一个事件或者一个系统，所有发生结果信息量的平均值，熵是针对整个系统而言
$$
H(X)=-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(p\left(x_{i}\right)\right)
$$
然而有一类比较特殊的问题，比如投掷硬币只有两种可能，字朝上或花朝上。买彩票只有两种可能，中奖或不中奖。我们称之为0-1分布问题（二项分布的特例），对于这类问题，熵的计算方法可以简化为如下算式：
$$
\begin{aligned} H(X) &=-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(p\left(x_{i}\right)\right) \\ &=-p(x) \log (p(x))-(1-p(x)) \log (1-p(x)) \end{aligned}
$$

#### 相对熵（KL散度）

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异 ，并且它是不对称的，

D(P||Q)不等于D(Q||P)

维基百科对相对熵的定义 

如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。 

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]  直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。 

KL散度的计算公式： 
$$
D_{K L}(p \| q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right)
$$
n为事件的所有可能性。  DKL的值越小，表示q分布和p分布越接近 



#### 交叉熵

对式11变形可以得到 ，
$$
\begin{aligned} D_{K L}(p \| q) &=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(p\left(x_{i}\right)\right)-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right) \\ &=-H(p(x))+\left[-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)\right] \end{aligned}
$$
等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵： 
$$
H(p, q)=-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)
$$
在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即**$D_{KL}(y||\hat{y})$**，由于KL散度中的前一部分−H(y)−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

