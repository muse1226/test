#  线性回归

## 单变量线性回归

线性回归假设特征与结果满足线性关系。每个特征对结果的影响强弱可以由前面的参数体现。线性模型表现为：
$$
y=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\cdots+\theta_{n} x_{n}
$$



$\theta$反映x特征对y的影响程度，因此，我们可以通过每个自变量特征）前面的参数，可以很直观的看出那些特征分量对结果的影响比较大。

如果令 x0=1，y=$h_{\theta}(x)$，可以将上述模型写成向量量形式，即：
$$
h_{\theta}(x)=\sum_{i=0}^{n} \theta_{i} x_{i}=\theta^{T} x
$$
在⼀一些应用场合中，需要将输⼊入空间映射到特征空间中，然后建模，定义映射
函数为 $\phi(x)$，因此我们可以把公式写成更更通⽤用的表达公式：
$$
h_{\theta}(x)=\theta^{T} \Phi(x)
$$
特征映射相关技术，包括特征哈希、特征学习、Kernel 等。

我们预测房屋的价格

m代表训练样本的个数

x代表输入的变量或者特征

y代表输出结果或者预测的结果

(x^i,y^i)代表第i个实例



把训练样本喂给学习算法后，学习到一个新的函数，也称为假设h（hypothesis），输入新的x，得到预测的y。对于房价问题，只有一个特征变量，因此为单变量线性回归问题。表达式可能为：
$$
h_{\theta}(x)=\theta_{0}+\theta_{1} x
$$

### 代价函数cost Function

我们如何找到一个合适的直线去拟合训练样本，使得它与样本的误差最小，也就是让代价函数（也称为平方误差函数）最小。
$$
J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$

为什什么要选用误差平⽅方和作为目标函数呢？答案可以从概率论中的中心极限定理、高斯分布等知识中找到



### 解决问题的思路

建立一个线性假设函数

$$
h_{\theta}(x)=\theta_{0}+\theta_{1} x
$$
如何选取最好的参数去拟合训练样本

$\theta_0,\theta_1$ 

利用代价函数求得最好的参数

$$
J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$
目标求代价函数最小值

$$
\underset{\theta_{0}, \theta_{1}}{\operatorname{minimize}} J\left(\theta_{0}, \theta_{1}\right)
$$


## 优化方法

### 梯度下降法

为了自动的，更快更好的找到代价函数的最小值，我们在线性回归利用了梯度下降法

#### **批量梯度下降法过程**

批量指的是梯度下降的每一步，都用到了所有的训练样本

总体思路：先随机选取一个初始点，然后沿着代价函数值下降最快的方向，也就是梯度方向，这样就找到一个新的点，再不断沿着梯度方向下降，直到找到一个局部最小值。

问题：1. 选取不同初始点，最后找到的局部最小值可能不同

#### 算法流程



$ \alpha$是学习率，决定了我们沿着让代价函数下降最快的方向迈出的步子有多大

注意有一点，计算出更新后的$\theta_0$后，不带入temp1中，temp1中的$\theta_0$仍然是未更新的

，这叫做同步更新

#### 选取学习率的问题

选择不同的学习率会导致不同的问题，学习率太小，步长太小，下降速度太慢，学习率太大，步长太大，容易跨过最小点，导致无法收敛，甚至发散。

**选取学习率的方法**

***Adagrad***

随着更新次数的增加，$\eta$减小，步长减小

$$
\eta^{t}=\frac{\eta}{\sqrt{t+1}}       
g^{t}=\frac{\partial L\left(\theta^{t}\right)}{\partial w}

w^{t+1} \leftarrow w^{t}-\frac{\eta^{t}}{\sigma^{t}} g^{t}

\sigma^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(g^{i}\right)^{2}}
$$
例如
$$
\sigma^{1}=\sqrt{\frac{1}{2}\left[\left(g^{0}\right)^{2}+\left(g^{1}\right)^{2}\right]}
$$
经过化简，也发现，步长与loss函数的一次偏导成正比，与二次偏导成反比，
$$
\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}
$$
与二次偏导进行了近似

$$
w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}
$$
$\eta$为常数，g为loss函数偏导，t为更新次数





***Adam***







$$
\theta_{j} :=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)

**梯度下降可以自动减小步长
$$
![1557041284679](C:\Users\ADMINI~1\AppData\Local\Temp\1557041284679.png)

如图，到绿色点后，斜率不断下降，学习率不变，步长自动就会减小，所以没必要再另外减小学习率。

一般超过2个参数后，就很难可视化梯度下降，但我们可以画出梯度几次后loss函数的走势。如下图

![1557055815079](C:\Users\ADMINI~1\AppData\Local\Temp\1557055815079.png)

通过这图，梯度几次后，大概判断loss走势，不然假如它走势一直增大，不断梯度就是在浪费时间



#### 梯度下降的缺点

1.容易陷入局部最优

### 随机梯度下降

### 梯度下降技巧

#### 特征缩放

使不同的特征有一样的规模
$$
y=b+w_{1} x_{1}+w_{2} x_{2}
$$
![1557061902320](C:\Users\ADMINI~1\AppData\Local\Temp\1557061902320.png)

如果x1和x2,两个特征数值差距很大，比如x1=1,2....,x2=100,200,造成x2对结果影响更大，此时它的loss函数如上图1，成椭圆形，**梯度下降时总是沿着登高线的法线方向**，所以它不会直接达到局部最小值，而特征缩放后，图2圆形，可以直接梯度到最小值，加快了速度。

#### 特征缩放的方法

### 梯度下降注意的数学问题

更新次数增大，loss可能越来越大，原因？？？？

由泰勒公式得，sin(x)在x=0附近，近似为一条直线，为一次函数。

同理，多元函数
$$
h(x, y) \approx h\left(x_{0}, y_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial x}\left(x-x_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial y}\left(y-y_{0}\right)
$$
![1557062708230](C:\Users\ADMINI~1\AppData\Local\Temp\1557062708230.png)

**当随机选一个初始点，当它邻域，为一个圆圈，足够小时，loss函数二次，三次，，及之后的项太小被忽略**，此时loss函数为
$$
\mathrm{L}(\theta) \approx \mathrm{L}(a, b)+\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{1}}\left(\theta_{1}-a\right)+\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{2}}\left(\theta_{2}-b\right)
$$
![1557062998774](C:\Users\ADMINI~1\AppData\Local\Temp\1557062998774.png)

我们此时在红圈中找到局部最小点，满足：
$$
\left(\theta_{1}-a\right)^{2}+\left(\theta_{2}-b\right)^{2} \leq d^{2}
$$

$$
\mathrm{L}(\theta) \approx s+u\left(\theta_{1}-a\right)+v\left(\theta_{2}-b\right)
$$

s为常数，其实就是后面的向量积最小，为了最小化loss函数，取$\Delta\theta$与（u,v)反方向

$$
\left[ \begin{array}{c}{\Delta \theta_{1}} \\ {\Delta \theta_{2}}\end{array}\right]=-\eta \left[ \begin{array}{l}{u} \\ {v}\end{array}\right]
$$

$$
\left[ \begin{array}{l}{\theta_{1}} \\ {\theta_{2}}\end{array}\right]=\left[ \begin{array}{l}{a} \\ {b}\end{array}\right]-\eta \left[ \begin{array}{l}{u} \\ {v}\end{array}\right]
$$



![1557063240074](C:\Users\ADMINI~1\AppData\Local\Temp\1557063240074.png)

**由此可见，要用梯度下降法的一个前提是，在一个极小的圆圈中，将loss函数近似化，为了取得这个圆圈中的最小值点，最终得出了梯度下降法参数选取的方法，学习率与半径d成正比，所以学习率一般选很小，所以学习率选择不对，可能会导致d大，导致loss函数不能近似，不能采用梯度下降法，这样做出来的结果loss就可能会变大**





### 梯度下降的线性回归

线性回归问题的代价函数通常为一个凸函数，没有局部最优，只有全局最优

![1557043051975](C:\Users\ADMINI~1\AppData\Local\Temp\1557043051975.png)











